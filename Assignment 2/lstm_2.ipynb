{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bb16bf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential # type: ignore\n",
    "from keras.layers import Conv1D, LSTM, Dropout, BatchNormalization, Dense, GlobalAveragePooling1D, TimeDistributed # type: ignore\n",
    "from keras.optimizers import Nadam # type: ignore\n",
    "from keras.callbacks import EarlyStopping # type: ignore\n",
    "from keras.utils import to_categorical # type: ignore\n",
    "from keras.regularizers import l2 # type: ignore\n",
    "from sklearn.utils import shuffle # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137313f",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f296ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNSAMPLING_STEP = 2\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = 400\n",
    "\n",
    "ENCODE_MAP = {\n",
    "        'rest': 0,\n",
    "        'motor': 1,\n",
    "        'memory': 2,\n",
    "        'math': 3,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a48b5",
   "metadata": {},
   "source": [
    "## Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f16469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_name(filename_with_dir):\n",
    "    filename_without_dir = str(filename_with_dir.name)\n",
    "    temp = filename_without_dir.split('_')[:-1]\n",
    "    dataset_name = '_'.join(temp)\n",
    "    # chunk_n = filename_without_dir.split('_').split('.')[0]\n",
    "    return dataset_name\n",
    "\n",
    "def extract_label(filename, logs=False, encode_mapping=ENCODE_MAP):\n",
    "    for key in encode_mapping:\n",
    "        if key in filename:\n",
    "            if logs: print(f\"Mapping label {filename} to {encode_mapping[key]}\")\n",
    "            return encode_mapping[key]\n",
    "    return encode_mapping['math']\n",
    "\n",
    "def load_all_data(folder_path, logs=False, batch_size=100, step=4):\n",
    "    folder_path = Path(folder_path)\n",
    "    X, y = [], []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.h5'):\n",
    "            print(f\"Loading {filename}\", end=\" \") if logs else None\n",
    "            file_path = folder_path / filename\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                dataset_name = get_dataset_name(file_path)\n",
    "                data = f.get(dataset_name)[()]  # shape (248, time)\n",
    "                data = data.T  # (time, 248)\n",
    "                if logs: print(f\"Original: {data.shape}\", end=\" \")\n",
    "\n",
    "                # --- Preprocess before batching ---\n",
    "                # Normalize\n",
    "                mean = np.mean(data, axis=0, keepdims=True)\n",
    "                std = np.std(data, axis=0, keepdims=True) + 1e-8\n",
    "                data = (data - mean) / std\n",
    "\n",
    "                # Downsample time dimension\n",
    "                data = data[::step, :]  # (downsampled_time, 248)\n",
    "                if logs: print(f\"Downsampled: {data.shape}\", end=\" \")\n",
    "\n",
    "                label = extract_label(filename, logs=0)\n",
    "                if logs: print(f\"Label: {label}\")\n",
    "\n",
    "                # X.append(data)  # shape (downsampled_time, 248)\n",
    "                # y.append(label)\n",
    "\n",
    "                num_batches = len(data) // batch_size\n",
    "                data_batches = np.split(data[:num_batches * batch_size], num_batches)\n",
    "                X += data_batches\n",
    "                y += [label] * num_batches\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(f\"Final shape of X: {X.shape}, y: {y.shape}\")\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9544501",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_all_data(\"Intra/train\", logs=0, batch_size=BATCH_SIZE, step=DOWNSAMPLING_STEP)\n",
    "print(\"Shape after preprocessing:\", X_train.shape)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "print(\"Shape after shuffling:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491ab1b",
   "metadata": {},
   "source": [
    "## LSTM Model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = X_train.shape[1]\n",
    "num_channels = X_train.shape[2]\n",
    "\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Define LSTM model\n",
    "lstm_model = Sequential([\n",
    "    TimeDistributed(Dense(128, kernel_regularizer=l2(1e-4)), input_shape=(time_steps, num_channels)),\n",
    "    \n",
    "    LSTM(128, return_sequences=True, kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    LSTM(64, kernel_regularizer=l2(1e-3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(1e-3)),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Nadam(learning_rate=0.0001),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75375aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lstm_model.fit(\n",
    "    X_train, y_train,   \n",
    "    epochs=5,\n",
    "    batch_size=20,\n",
    "    validation_split=0.5,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1fe6b",
   "metadata": {},
   "source": [
    "## Second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode if needed\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Split manually (50% validation)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train.argmax(axis=1))\n",
    "\n",
    "# Model definition\n",
    "time_steps = X_train.shape[1]\n",
    "num_channels = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64291e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Train labels distribution:\", np.bincount(y_train_split.argmax(axis=1)))\n",
    "print(\"Validation labels distribution:\", np.bincount(y_val_split.argmax(axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f67987",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "    Conv1D(64, kernel_size=1, activation='linear', input_shape=(time_steps, num_channels)),\n",
    "    \n",
    "    LSTM(128, return_sequences=True, kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    LSTM(64, kernel_regularizer=l2(1e-3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(1e-3)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(time_steps, num_channels)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer=Nadam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40541ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4b76a",
   "metadata": {},
   "source": [
    "## Testing and evaluating part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f209ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "X_test, y_test = load_all_data(\"Data/Intra/test\", logs=0)\n",
    "print(f\"X_test shape before preprocessing: {np.array(X_test).shape}\")\n",
    "\n",
    "X_test = z_score_normalize(X_test) # TODO DOWNSAMPLE\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "print(f\"X_test shape after preprocessing: {np.array(X_test).shape}\")\n",
    "\n",
    "# One-hot encode labels\n",
    "y_test = to_categorical(y_test, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss, test_acc = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84422ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim((0,1))\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities\n",
    "y_pred_probs = lstm_model.predict(X_test)\n",
    "# Convert one-hot back to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"rest\", \"motor\", \"memory\", \"math\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db387acd",
   "metadata": {},
   "source": [
    "# Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c680901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "X_test_cross, y_test_cross = load_all_data(\"Data/Cross/test1/\", logs=0)\n",
    "print(f\"X_test shape before preprocessing: {np.array(X_test_cross).shape}\")\n",
    "\n",
    "X_test_cross = z_score_normalize(X_test_cross) # TODO DOWNSAMPLE\n",
    "\n",
    "X_test_cross = np.array(X_test_cross)\n",
    "print(f\"X_test_cross shape after preprocessing: {np.array(X_test_cross).shape}\")\n",
    "\n",
    "# One-hot encode labels\n",
    "y_test_cross = to_categorical(y_test_cross, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss_cross, test_acc_cross = lstm_model.evaluate(X_test_cross, y_test_cross, verbose=1)\n",
    "print(\"Test accuracy:\", test_acc_cross)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
